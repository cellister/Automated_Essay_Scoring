{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1mkurPLHZxZLxUe1tBccAKnfpZH78xn8I\" alt=\"other_image\" style=\"width: 100px; display: block; margin: center;\">"
      ],
      "metadata": {
        "id": "RBdtiLHLnIGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1kUyDguzIG2FpKpm2Pvy2t231ITDentxd\" alt=\"other_image\" style=\"width: 100px; display: block; margin: center;\">"
      ],
      "metadata": {
        "id": "MdM3ZzIRMW9j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1azlBezTXslX"
      },
      "outputs": [],
      "source": [
        "#X_bow_array_df = pd.DataFrame(X_bow_array)\n",
        "#X_bow_array_df\n",
        "#value_summary = X_bow_array_df['Value'].describe()\n",
        "#print(value_summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index in random_indices:\n",
        "    print(\"Row Index:\", index)\n",
        "    print(\"\\nPreprocessed Text:\")\n",
        "    print(essay_df.loc[index, 'preprocessed'])\n",
        "    print(\"\\nBOWVector dictionary:\")\n",
        "    print(essay_df.loc[index, 'BOWVector'])\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")  # Add a separator between rows"
      ],
      "metadata": {
        "id": "BOXiQvL2o3Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "essay = \"The Quick 10% of two 1,000 Foxes, Jumped! can't 1.00 won't wouldn't : would've I'm she's. 1982 (044). well-being model -T position-estimating\""
      ],
      "metadata": {
        "id": "E4a16Khgo3Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "essay = essay.replace(\"-\", \" \")\n",
        "essay = essay.replace(\"%\", \" percent \")"
      ],
      "metadata": {
        "id": "U-q_4hcNo3Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "essay_expanded = contractions.fix(essay)\n",
        "essay_expanded"
      ],
      "metadata": {
        "id": "b1peTbFco3Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(essay_expanded) # just splitting on spaces\n",
        "tokens"
      ],
      "metadata": {
        "id": "IqRtC1NSpAQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contains_numeric(token):\n",
        "    return any(char.isdigit() for char in token)\n",
        "\n",
        "def convert_numbers_to_words(token):\n",
        "    if contains_numeric(token):\n",
        "        # Remove any punctuation from the token before conversion\n",
        "        token = re.sub(r'[^\\w\\s.]', '', token)\n",
        "\n",
        "        wordString = p.number_to_words(token)\n",
        "\n",
        "        wordString = wordString.replace(\"-\", \" \")\n",
        "\n",
        "        # Tokenize the wordString\n",
        "        tokens = word_tokenize(wordString)\n",
        "\n",
        "        return tokens  # Return the flat list of tokens instead of a list of lists\n",
        "    else:\n",
        "        return [token]  # Return the token as a list"
      ],
      "metadata": {
        "id": "lecpx7U7pATv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processedTokens = []\n",
        "\n",
        "for token in tokens:\n",
        "    processedTokens.extend(convert_numbers_to_words(token))\n",
        "\n",
        "processedTokens"
      ],
      "metadata": {
        "id": "TpLvbKMQpAWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation and convert to lowercase\n",
        "essayTokens = [word.lower() for word in processedTokens if word.isalpha()]\n",
        "essayTokens"
      ],
      "metadata": {
        "id": "HFlMK0fbpAZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally, remove stopwords (common words like 'the', 'and', etc.)\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "essayTokensClean = [word for word in essayTokens if word not in stop_words]\n",
        "essayTokensClean"
      ],
      "metadata": {
        "id": "VQJjJzA_pMEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other options before vectorizing: stemming, lemmatization, stop words (which we already did), other types of token manipulations (might include context), spelling consideration..."
      ],
      "metadata": {
        "id": "WYY73pdqpMN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VECTORIZE!\n",
        "\n",
        "bowRepresentation = Counter(essayTokensClean)\n",
        "bowRepresentation"
      ],
      "metadata": {
        "id": "4OSwKRWrpMXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "essay2 = \"The Quick 10% of two 1,000 Foxes, Jumped! can't 1.00 won't wouldn't : would've I'm she's. 1982 (044). well-being model -T position-estimating\""
      ],
      "metadata": {
        "id": "jlWL3LwUpMeZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}